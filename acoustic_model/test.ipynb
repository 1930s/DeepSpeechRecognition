{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "\n",
    "# 获取信号的时频图\n",
    "def compute_fbank(file):\n",
    "\tx=np.linspace(0, 400 - 1, 400, dtype = np.int64)\n",
    "\tw = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (400 - 1) ) # 汉明窗\n",
    "\tfs, wavsignal = wav.read(file)\n",
    "\t# wav波形 加时间窗以及时移10ms\n",
    "\ttime_window = 25 # 单位ms\n",
    "\twindow_length = fs / 1000 * time_window # 计算窗长度的公式，目前全部为400固定值\n",
    "\twav_arr = np.array(wavsignal)\n",
    "\twav_length = len(wavsignal)\n",
    "\trange0_end = int(len(wavsignal)/fs*1000 - time_window) // 10 # 计算循环终止的位置，也就是最终生成的窗数\n",
    "\tdata_input = np.zeros((range0_end, 200), dtype = np.float) # 用于存放最终的频率特征数据\n",
    "\tdata_line = np.zeros((1, 400), dtype = np.float)\n",
    "\tfor i in range(0, range0_end):\n",
    "\t\tp_start = i * 160\n",
    "\t\tp_end = p_start + 400\n",
    "\t\tdata_line = wav_arr[p_start:p_end]\t\n",
    "\t\tdata_line = data_line * w # 加窗\n",
    "\t\tdata_line = np.abs(fft(data_line))\n",
    "\t\tdata_input[i]=data_line[0:200] # 设置为400除以2的值（即200）是取一半数据，因为是对称的\n",
    "\tdata_input = np.log(data_input + 1)\n",
    "\t#data_input = data_input[::]\n",
    "\treturn data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def source_get(source_file):\n",
    "    train_file = source_file + '\\\\train'\n",
    "    label_lst = []\n",
    "    wav_lst = []\n",
    "    for root, dirs, files in os.walk(train_file):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav') or file.endswith('.WAV'):\n",
    "                wav_file = os.sep.join([root, file])\n",
    "                wav_lst.append(wav_file)\n",
    "            elif file.endswith('.trn'):\n",
    "                label_file = os.sep.join([source_file, 'data', file])\n",
    "                label_lst.append(label_file)\n",
    "    return label_lst, wav_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = 'E:\\\\Data\\\\thchs30\\\\data_thchs30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst, wav_lst = source_get(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de5 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n",
      "\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def read_label(label_file):\n",
    "    with open(label_file, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        return data[1]\n",
    "\n",
    "print(read_label(label_lst[0]))\n",
    "\n",
    "def gen_label_data(label_lst):\n",
    "    label_data = []\n",
    "    for label_file in label_lst:\n",
    "        pny = read_label(label_file)\n",
    "        label_data.append(pny.strip('\\n'))\n",
    "    return label_data\n",
    "\n",
    "label_data = gen_label_data(label_lst)\n",
    "print(len(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n"
     ]
    }
   ],
   "source": [
    "def mk_vocab(label_data):\n",
    "    vocab = []\n",
    "    for line in label_data:\n",
    "        line = line.split(' ')\n",
    "        for pny in line:\n",
    "            if pny not in vocab:\n",
    "                vocab.append(pny)\n",
    "    vocab.append('_')\n",
    "    return vocab\n",
    "\n",
    "vocab = mk_vocab(label_data)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de5 di3 se4 si4 yue4 de5 lin2 luan2 geng4 shi4 lv4 de5 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 10, 15, 16, 17, 1, 0, 10, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "def word2id(line, vocab):\n",
    "    return [vocab.index(pny) for pny in line.split(' ')]\n",
    "\n",
    "label_id = word2id(label_data[0], vocab)\n",
    "print(label_data[0])\n",
    "print(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle_list = [i for i in range(10000)]\n",
    "shuffle(shuffle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, shuffle_list, wav_lst, label_data, vocab):\n",
    "    for i in range(10000//batch_size):\n",
    "        wav_data_lst = []\n",
    "        label_data_lst = []\n",
    "        begin = i * batch_size\n",
    "        end = begin + batch_size\n",
    "        sub_list = shuffle_list[begin:end]\n",
    "        for index in sub_list:\n",
    "            fbank = compute_fbank(wav_lst[index])\n",
    "            fbank = fbank[:fbank.shape[0] // 8 * 8, :]\n",
    "            label = word2id(label_data[index], vocab)\n",
    "            wav_data_lst.append(fbank)\n",
    "            label_data_lst.append(label)\n",
    "        yield wav_data_lst, label_data_lst\n",
    "\n",
    "batch = get_batch(4, shuffle_list, wav_lst, label_data, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1016, 200)\n",
      "(960, 200)\n",
      "(744, 200)\n",
      "(784, 200)\n",
      "[148, 386, 727, 272, 865, 265, 416, 123, 14, 237, 69, 152, 33, 501, 105, 307, 42, 56, 568, 148, 386, 695, 338, 86, 246, 994, 628, 727, 272, 416, 262, 336, 95, 101, 1]\n",
      "[103, 280, 170, 35, 424, 599, 90, 651, 223, 495, 10, 150, 28, 121, 144, 319, 634, 662, 342, 227, 540, 48, 401, 367, 246, 134, 339, 570, 570, 639, 196, 163, 0]\n",
      "[999, 90, 237, 18, 999, 87, 243, 504, 237, 58, 523, 688, 91, 394, 999, 91, 243, 237, 58, 523, 688, 717, 394]\n",
      "[113, 526, 94, 384, 590, 11, 848, 2, 967, 190, 888, 73, 1167, 1167, 707, 309, 212, 354, 223, 229, 155, 768]\n"
     ]
    }
   ],
   "source": [
    "wav_data_lst, label_data_lst = next(batch)\n",
    "for wav_data in wav_data_lst:\n",
    "    print(wav_data.shape)\n",
    "for label_data in label_data_lst:\n",
    "    print(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n",
      "[1016, 960, 744, 784]\n"
     ]
    }
   ],
   "source": [
    "lens = [len(wav) for wav in wav_data_lst]\n",
    "print(max(lens))\n",
    "print(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1016, 200, 1)\n",
      "[127 120  93  98]\n"
     ]
    }
   ],
   "source": [
    "def wav_padding(wav_data_lst):\n",
    "    wav_lens = [len(data) for data in wav_data_lst]\n",
    "    wav_max_len = max(wav_lens)\n",
    "    wav_lens = np.array([leng//8 for leng in wav_lens])\n",
    "    new_wav_data_lst = np.zeros((len(wav_data_lst), wav_max_len, 200, 1))\n",
    "    for i in range(len(wav_data_lst)):\n",
    "        new_wav_data_lst[i, :wav_data_lst[i].shape[0], :, 0] = wav_data_lst[i]\n",
    "    return new_wav_data_lst, wav_lens\n",
    "\n",
    "pad_wav_data_lst, wav_lens = wav_padding(wav_data_lst)\n",
    "print(pad_wav_data_lst.shape)\n",
    "print(wav_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 35)\n",
      "[35 33 23 22]\n"
     ]
    }
   ],
   "source": [
    "def label_padding(label_data_lst):\n",
    "    label_lens = np.array([len(label) for label in label_data_lst])\n",
    "    max_label_len = max(label_lens)\n",
    "    new_label_data_lst = np.zeros((len(label_data_lst), max_label_len))\n",
    "    for i in range(len(label_data_lst)):\n",
    "        new_label_data_lst[i][:len(label_data_lst[i])] = label_data_lst[i]\n",
    "    return new_label_data_lst, label_lens\n",
    "\n",
    "pad_label_data_lst, label_lens = label_padding(label_data_lst)\n",
    "print(pad_label_data_lst.shape)\n",
    "print(label_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nums = 10000\n",
    "batch_size = 4\n",
    "batch_num = total_nums // batch_size\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = 'E:\\\\Data\\\\thchs30\\\\data_thchs30'\n",
    "label_lst, wav_lst = source_get(source_file)\n",
    "label_data = gen_label_data(label_lst)\n",
    "vocab = mk_vocab(label_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "shuffle_list = [i for i in range(10000)]\n",
    "shuffle(shuffle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab):\n",
    "    for i in range(10000//batch_size):\n",
    "        wav_data_lst = []\n",
    "        label_data_lst = []\n",
    "        begin = i * batch_size\n",
    "        end = begin + batch_size\n",
    "        sub_list = shuffle_list[begin:end]\n",
    "        for index in sub_list:\n",
    "            fbank = compute_fbank(wav_lst[index])\n",
    "            fbank = fbank[:fbank.shape[0] // 8 * 8, :]\n",
    "            label = word2id(label_data[index], vocab)\n",
    "            wav_data_lst.append(fbank)\n",
    "            label_data_lst.append(label)\n",
    "        pad_wav_data, input_length = wav_padding(wav_data_lst)\n",
    "        pad_label_data, label_length = label_padding(label_data_lst)\n",
    "        inputs = {'the_inputs': pad_wav_data,\n",
    "                  'the_labels': pad_label_data,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                 }\n",
    "        outputs = {'ctc': np.zeros(pad_wav_data.shape[0],)} \n",
    "        yield inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input, Reshape, BatchNormalization\n",
    "from keras.layers import Lambda, Activation,Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "\n",
    "\n",
    "class ModelSpeech(): # 语音模型类\n",
    "\tdef __init__(self, vocab_size):\n",
    "\t\tself.MS_OUTPUT_SIZE = vocab_size\n",
    "\t\tself.label_max_string_length = 64\n",
    "\t\tself.AUDIO_FEATURE_LENGTH = 200\n",
    "\t\tself._model, self.base_model = self.CreateModel()\n",
    "\n",
    "\t\t\n",
    "\tdef CreateModel(self):\n",
    "\t\t# 每一帧使用13维mfcc特征及其13维一阶差分和13维二阶差分表示，最大信号序列长度为1500\n",
    "\t\tinput_data = Input(name='the_inputs', shape=(None, 200, 1))\n",
    "\t\t\n",
    "\t\tlayer_h1 = Conv2D(32, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(input_data) # 卷积层\n",
    "\t\tlayer_h1 = BatchNormalization(mode=0,axis=-1)(layer_h1)\n",
    "\t\tlayer_h2 = Conv2D(32, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h1) # 卷积层\n",
    "\t\tlayer_h3 = MaxPooling2D(pool_size=2, strides=None, padding=\"valid\")(layer_h2) # 池化层\n",
    "\t\t#layer_h3 = Dropout(0.2)(layer_h2) # 随机中断部分神经网络连接，防止过拟合\n",
    "\t\tlayer_h3 = BatchNormalization(mode=0,axis=-1)(layer_h3)\n",
    "\t\tlayer_h4 = Conv2D(64, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h3) # 卷积层\n",
    "\t\tlayer_h4 = BatchNormalization(mode=0,axis=-1)(layer_h4)\n",
    "\t\tlayer_h5 = Conv2D(64, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h4) # 卷积层\n",
    "\t\tlayer_h6 = MaxPooling2D(pool_size=2, strides=None, padding=\"valid\")(layer_h5) # 池化层\n",
    "\t\t\n",
    "\t\tlayer_h6 = BatchNormalization(mode=0,axis=-1)(layer_h6)\n",
    "\t\tlayer_h7 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h6) # 卷积层\n",
    "\t\tlayer_h7 = BatchNormalization(mode=0,axis=-1)(layer_h7)\n",
    "\t\tlayer_h8 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h7) # 卷积层\n",
    "\t\tlayer_h9 = MaxPooling2D(pool_size=2, strides=None, padding=\"valid\")(layer_h8) # 池化层\n",
    "\t\t\n",
    "\t\tlayer_h9 = BatchNormalization(mode=0,axis=-1)(layer_h9)\n",
    "\t\tlayer_h10 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h9) # 卷积层\n",
    "\t\tlayer_h10 = BatchNormalization(mode=0,axis=-1)(layer_h10)\n",
    "\t\tlayer_h11 = Conv2D(128, (3,3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal')(layer_h10) # 卷积层\n",
    "\t\tlayer_h12 = MaxPooling2D(pool_size=1, strides=None, padding=\"valid\")(layer_h11) # 池化层\n",
    "\t\t\n",
    "\t\t#test=Model(inputs = input_data, outputs = layer_h12)\n",
    "\t\t#test.summary()\n",
    "\t\t\n",
    "\t\tlayer_h10 = Reshape((-1, 3200))(layer_h12) #Reshape层\n",
    "\t\t#layer_h5 = LSTM(256, activation='relu', use_bias=True, return_sequences=True)(layer_h4) # LSTM层\n",
    "\t\t#layer_h6 = Dropout(0.2)(layer_h5) # 随机中断部分神经网络连接，防止过拟合\n",
    "\t\tlayer_h10 = BatchNormalization(mode=0,axis=-1)(layer_h10)\n",
    "\t\tlayer_h11 = Dense(128, activation=\"relu\", use_bias=True, kernel_initializer='he_normal')(layer_h10) # 全连接层\n",
    "\t\tlayer_h11 = BatchNormalization(mode=0,axis=-1)(layer_h11)\n",
    "\t\tlayer_h12 = Dense(self.MS_OUTPUT_SIZE, use_bias=True, kernel_initializer='he_normal')(layer_h11) # 全连接层\n",
    "\t\t\n",
    "\t\ty_pred = Activation('softmax', name='Activation0')(layer_h12)\n",
    "\t\tmodel_data = Model(inputs = input_data, outputs = y_pred)\n",
    "\t\t#model_data.summary()\n",
    "\t\t\n",
    "\t\tlabels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "\t\tinput_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "\t\tlabel_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\t\t# Keras doesn't currently support loss funcs with extra parameters\n",
    "\t\t# so CTC loss is implemented in a lambda layer\n",
    "\t\t\n",
    "\t\t#layer_out = Lambda(ctc_lambda_func,output_shape=(self.MS_OUTPUT_SIZE, ), name='ctc')([y_pred, labels, input_length, label_length])#(layer_h6) # CTC\n",
    "\t\tloss_out = Lambda(self.ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tmodel = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\t\t\n",
    "\t\tmodel.summary()\n",
    "\t\t\n",
    "\t\t# clipnorm seems to speeds up convergence\n",
    "\t\t#sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\t\tada_d = Adadelta(lr = 0.01, rho = 0.95, epsilon = 1e-06)\n",
    "\t\t\n",
    "\t\tmodel.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = ada_d)\n",
    "\t\tprint('[*提示] 创建模型成功，模型编译成功')\n",
    "\t\treturn model, model_data\n",
    "\t\t\n",
    "\tdef ctc_lambda_func(self, args):\n",
    "\t\ty_pred, labels, input_length, label_length = args\n",
    "\t\t\n",
    "\t\ty_pred = y_pred[:, :, :]\n",
    "\t\t#y_pred = y_pred[:, 2:, :]\n",
    "\t\treturn K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tdef TrainModel(self, yielddatas, epoch = 2, save_step = 1000,filename = 'model_speech/speech_model25'):\n",
    "\t\tfor epoch in range(epoch): # 迭代轮数\n",
    "\t\t\tprint('[running] train epoch %d .' % epoch)\n",
    "\t\t\tn_step = 0 # 迭代数据数\n",
    "\t\t\twhile True:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tprint('[message] epoch %d . Have train datas %d+'%(epoch, n_step*save_step))\n",
    "\t\t\t\t\t# data_genetator是一个生成器函数\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#self._model.fit_generator(yielddatas, save_step, nb_worker=2)\n",
    "\t\t\t\t\tself._model.fit_generator(yielddatas, save_step)\n",
    "\t\t\t\t\tn_step += 1\n",
    "\t\t\t\texcept StopIteration:\n",
    "\t\t\t\t\tprint('[error] generator error. please check data format.')\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.SaveModel(comment='_e_'+str(epoch)+'_step_'+str(n_step * save_step))\n",
    "\t\t\t\tself.TestModel(self.datapath, str_dataset='train', data_count = 4)\n",
    "\t\t\t\tself.TestModel(self.datapath, str_dataset='dev', data_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_inputs (InputLayer)         (None, None, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, 200, 32 320         the_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 200, 32 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, 200, 32 9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, 100, 32 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 100, 32 128         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, 100, 64 18496       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100, 64 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, 100, 64 36928       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, 50, 64) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 50, 64) 256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, 50, 128 73856       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 50, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, 50, 128 147584      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, 25, 128 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 25, 128 512         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, 25, 128 147584      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 25, 128 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, 25, 128 147584      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, 25, 128 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 3200)   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 3200)   12800       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 128)    409728      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 128)    512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1176)   151704      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Activation0 (Activation)        (None, None, 1176)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           Activation0[0][0]                \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,158,648\n",
      "Trainable params: 1,150,840\n",
      "Non-trainable params: 7,808\n",
      "__________________________________________________________________________________________________\n",
      "[*提示] 创建模型成功，模型编译成功\n"
     ]
    }
   ],
   "source": [
    "am = ModelSpeech(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[running] train epoch 0 .\n",
      "[message] epoch 0 . Have train datas 0+\n",
      "Epoch 1/1\n",
      "  91/1000 [=>............................] - ETA: 3:16:03 - loss: 388.3666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cb81ecc985ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-211ceab78f98>\u001b[0m in \u001b[0;36mTrainModel\u001b[1;34m(self, yielddatas, epoch, save_step, filename)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                                         \u001b[1;31m#self._model.fit_generator(yielddatas, save_step, nb_worker=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myielddatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                                         \u001b[0mn_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2667\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2649\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2650\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "am.TrainModel(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_units = 512\n",
    "T = 10\n",
    "position_enc = np.array([\n",
    "            [pos / np.power(10000, 2.*i / num_units) for i in range(num_units)]\n",
    "            for pos in range(T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " position_enc[:,0::2] = np.sin(position_enc[:,0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_enc[:,1::2] = np.cos(position_enc[:,1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470985e-01  5.69695009e-01  8.01961795e-01 ...  1.00000000e+00\n",
      "   1.07460783e-08  1.00000000e+00]\n",
      " [ 9.09297427e-01 -3.50895194e-01  9.58144376e-01 ...  1.00000000e+00\n",
      "   2.14921566e-08  1.00000000e+00]\n",
      " ...\n",
      " [ 6.56986599e-01  8.91819036e-01  2.28774860e-01 ...  1.00000000e+00\n",
      "   7.52225480e-08  1.00000000e+00]\n",
      " [ 9.89358247e-01  1.36263428e-01  9.17357711e-01 ...  1.00000000e+00\n",
      "   8.59686263e-08  1.00000000e+00]\n",
      " [ 4.12118485e-01 -7.36561846e-01  8.67238862e-01 ...  1.00000000e+00\n",
      "   9.67147045e-08  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(position_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.0\n",
      "266.24185030469096\n",
      "266.98979647175173\n",
      "260.11838541244845\n",
      "250.7842798085775\n",
      "244.02486120776146\n",
      "241.8208395336924\n",
      "242.59022044153295\n",
      "243.16969720376335\n",
      "241.51934277045626\n"
     ]
    }
   ],
   "source": [
    "for line in position_enc:\n",
    "    x = 0\n",
    "    for i in line:\n",
    "        x+=i\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 1, 3)\n",
      "(2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1,0,0], [1,1,0]])\n",
    "keys = np.zeros((2,3,4))\n",
    "key_masks = np.sign(np.abs(np.sum(keys,axis=-1)))\n",
    "print(key_masks.shape)\n",
    "print(np.expand_dims(key_masks,1).shape)\n",
    "key_masks = np.tile(np.expand_dims(key_masks,1),[1,np.shape(keys)[1],1])\n",
    "print(key_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(key_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 1, 3)\n",
      "[[[1 0 0]\n",
      "  [1 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[1 1 0]\n",
      "  [1 1 0]\n",
      "  [1 1 0]]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1,0,0], [1,1,0]])\n",
    "key_masks = np.sign(np.abs(inputs))\n",
    "print(key_masks.shape)\n",
    "print(np.expand_dims(key_masks,1).shape)\n",
    "key_masks = np.tile(np.expand_dims(key_masks,1),[1,np.shape(keys)[1],1])\n",
    "print(key_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

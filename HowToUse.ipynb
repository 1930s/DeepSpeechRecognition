{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 声学模型训练\n",
    "\n",
    "train.py文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get source list...\n",
      "load  thchs_train.txt  data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 236865.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load  aishell_train.txt  data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120098/120098 [00:00<00:00, 260863.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make am vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9986.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make lm pinyin vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9946.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make lm hanzi vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9950.90it/s]\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_inputs (InputLayer)      (None, None, 200, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, 200, 32)     320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 200, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, 200, 32)     9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 200, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, 100, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, 100, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 100, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, 100, 64)     36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 100, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, None, 50, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, 50, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 50, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, 50, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 50, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, None, 25, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, None, 3200)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 230)         59110     \n",
      "=================================================================\n",
      "Total params: 1,759,174\n",
      "Trainable params: 1,757,254\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "load acoustic model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import get_data, data_hparams\n",
    "\n",
    "\n",
    "# 准备训练所需数据\n",
    "data_args = data_hparams()\n",
    "data_args.data_length = 10\n",
    "train_data = get_data(data_args)\n",
    "\n",
    "\n",
    "# 1.声学模型训练-----------------------------------\n",
    "from model_speech.cnn_ctc import Am, am_hparams\n",
    "am_args = am_hparams()\n",
    "am_args.vocab_size = len(train_data.am_vocab)\n",
    "am = Am(am_args)\n",
    "if os.path.exists('logs_am/model.h5'):\n",
    "    print('load acoustic model...')\n",
    "    am.ctc_model.load_weights('logs_am/model.h5')\n",
    "\n",
    "epochs = 0\n",
    "batch_num = len(train_data.wav_lst) // train_data.batch_size\n",
    "\n",
    "for k in range(epochs):\n",
    "    print('this is the', k+1, 'th epochs trainning !!!')\n",
    "    #shuffle(shuffle_list)\n",
    "    batch = train_data.get_am_batch()\n",
    "    am.ctc_model.fit_generator(batch, steps_per_epoch=batch_num, epochs=1)\n",
    "\n",
    "am.ctc_model.save_weights('logs_am/model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语言模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model...\n",
      "INFO:tensorflow:Restoring parameters from logs_lm/model\n"
     ]
    }
   ],
   "source": [
    "# 2.语言模型训练-------------------------------------------\n",
    "from model_language.transformer import Lm, lm_hparams\n",
    "\n",
    "\n",
    "lm_args = lm_hparams()\n",
    "lm_args.input_vocab_size = len(train_data.pny_vocab)\n",
    "lm_args.label_vocab_size = len(train_data.han_vocab)\n",
    "lm = Lm(lm_args)\n",
    "\n",
    "epochs = 0\n",
    "with lm.graph.as_default():\n",
    "    saver =tf.train.Saver()\n",
    "with tf.Session(graph=lm.graph) as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if os.path.exists('logs_lm/model.meta'):\n",
    "        print('loading language model...')\n",
    "        saver.restore(sess, 'logs_lm/model')\n",
    "    writer = tf.summary.FileWriter('logs_lm/tensorboard', tf.get_default_graph())\n",
    "    for k in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch = train_data.get_lm_batch()\n",
    "        for i in range(batch_num):\n",
    "            input_batch, label_batch = next(batch)\n",
    "            feed = {lm.x: input_batch, lm.y: label_batch}\n",
    "            cost,_ = sess.run([lm.mean_loss,lm.train_op], feed_dict=feed)\n",
    "            total_loss += cost\n",
    "            if (k * batch_num + i) % 10 == 0:\n",
    "                rs=sess.run(merged, feed_dict=feed)\n",
    "                writer.add_summary(rs, k * batch_num + i)\n",
    "        if (k+1) % 5 == 0:\n",
    "            print('epochs', k+1, ': average loss = ', total_loss/batch_num)\n",
    "    saver.save(sess, 'logs_lm/model')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型测试\n",
    "整合声学模型和语言模型\n",
    "\n",
    "test.py文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "# 定义解码器------------------------------------\n",
    "def decode_ctc(num_result, num2word):\n",
    "\tresult = num_result[:, :, :]\n",
    "\tin_len = np.zeros((1), dtype = np.int32)\n",
    "\tin_len[0] = result.shape[1]\n",
    "\tr = K.ctc_decode(result, in_len, greedy = True, beam_width=10, top_paths=1)\n",
    "\tr1 = K.get_value(r[0][0])\n",
    "\tr1 = r1[0]\n",
    "\ttext = []\n",
    "\tfor i in r1:\n",
    "\t\ttext.append(num2word[i])\n",
    "\treturn r1, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get source list...\n",
      "load  thchs_train.txt  data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 226097.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load  aishell_train.txt  data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120098/120098 [00:00<00:00, 226827.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make am vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9950.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make lm pinyin vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make lm hanzi vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9953.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# 0.准备解码所需数据------------------------------\n",
    "from utils import get_data, data_hparams\n",
    "data_args = data_hparams()\n",
    "data_args.data_length = 10\n",
    "train_data = get_data(data_args)\n",
    "\n",
    "am_batch = train_data.get_am_batch()\n",
    "lm_batch = train_data.get_lm_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载声学模型和语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_inputs (InputLayer)      (None, None, 200, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, None, 200, 32)     320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, None, 200, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, None, 200, 32)     9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, None, 200, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, None, 100, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, None, 100, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, None, 100, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, None, 100, 64)     36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, None, 100, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, None, 50, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, None, 50, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, None, 50, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, None, 50, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, None, 50, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, None, 25, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, None, 25, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, None, 25, 128)     512       \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, None, 3200)        0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 230)         59110     \n",
      "=================================================================\n",
      "Total params: 1,759,174\n",
      "Trainable params: 1,757,254\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "loading acoustic model...\n",
      "loading language model...\n",
      "INFO:tensorflow:Restoring parameters from logs_lm/model\n"
     ]
    }
   ],
   "source": [
    "# 1.声学模型-----------------------------------\n",
    "from model_speech.cnn_ctc import Am, am_hparams\n",
    "\n",
    "am_args = am_hparams()\n",
    "am_args.vocab_size = len(train_data.am_vocab)\n",
    "am = Am(am_args)\n",
    "print('loading acoustic model...')\n",
    "am.ctc_model.load_weights('logs_am/model.h5')\n",
    "\n",
    "# 2.语言模型-------------------------------------------\n",
    "from model_language.transformer import Lm, lm_hparams\n",
    "\n",
    "lm_args = lm_hparams()\n",
    "lm_args.input_vocab_size = len(train_data.pny_vocab)\n",
    "lm_args.label_vocab_size = len(train_data.han_vocab)\n",
    "print('loading language model...')\n",
    "lm = Lm(lm_args)\n",
    "sess = tf.Session(graph=lm.graph)\n",
    "with lm.graph.as_default():\n",
    "    saver =tf.train.Saver()\n",
    "with sess.as_default():\n",
    "    saver.restore(sess, 'logs_lm/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用语音识别系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the  0 th example.\n",
      "WARNING:tensorflow:From c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4303: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "文本结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n",
      "原文结果： lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2\n",
      "原文汉字： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然\n",
      "识别结果： 绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然\n",
      "\n",
      " the  1 th example.\n",
      "文本结果： ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1\n",
      "原文结果： ta1 jin3 ping2 yao1 bu4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1\n",
      "原文汉字： 他仅凭腰部的力量在泳道上下翻腾蛹动蛇行状如海豚一直以一头的优势领先\n",
      "识别结果： 他仅凭腰部的力量在泳道上下翻腾蛹动蛇行状如海豚一直以一头的优势领先\n",
      "\n",
      " the  2 th example.\n",
      "文本结果： pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4\n",
      "原文结果： pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shu1 di4 tuo1 qu4 yi1 fu2 guang1 bang3 zi chong1 jin4 le shui3 cuan4 dong4\n",
      "原文汉字： 炮眼打好了炸药怎么装岳正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞\n",
      "识别结果： 炮眼打好了炸药怎么装岳正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞\n",
      "\n",
      " the  3 th example.\n",
      "文本结果： ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 xian3 bu2 dui4 cheng1\n",
      "原文结果： ke3 shei2 zhi1 wen2 wan2 hou4 ta1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu1 you4 hei1 yu3 you4 ce4 ming2 xian3 bu2 dui4 cheng1\n",
      "原文汉字： 可谁知纹完后她一照镜子只见左下眼睑的线又粗又黑与右侧明显不对称\n",
      "识别结果： 可谁知纹完后她一照镜子知见左下眼睑的线右粗右黑与右侧明显不对称\n",
      "\n",
      " the  4 th example.\n",
      "文本结果： yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3\n",
      "原文结果： yi1 jin4 men2 wo3 bei4 jing1 dai1 le zhe4 hu4 ming2 jiao4 pang2 ji2 de lao3 nong2 shi4 kang4 mei3 yuan2 chao2 fu4 shang1 hui2 xiang1 de lao3 bing1 qi1 zi3 chang2 nian2 you3 bing4 jia1 tu2 si4 bi4 yi1 pin2 ru2 xi3\n",
      "原文汉字： 一进门我被惊呆了这户名叫庞吉的老农是抗美援朝负伤回乡的老兵妻子长年有病家徒四壁一贫如洗\n",
      "识别结果： 一进门我被惊呆了这户名叫庞吉的老农是抗美援朝负伤回乡的老兵妻子长年有病家徒四壁一贫如洗\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print('\\n the ', i, 'th example.')\n",
    "    # 载入训练好的模型，并进行识别\n",
    "    inputs, outputs = next(am_batch)\n",
    "    x = inputs['the_inputs']\n",
    "    y = inputs['the_labels'][0]\n",
    "    result = am.model.predict(x, steps=1)\n",
    "    # 将数字结果转化为文本结果\n",
    "    _, text = decode_ctc(result, train_data.am_vocab)\n",
    "    text = ' '.join(text)\n",
    "    print('文本结果：', text)\n",
    "    print('原文结果：', ' '.join([train_data.am_vocab[int(i)] for i in y]))\n",
    "    with sess.as_default():\n",
    "        _, y = next(lm_batch)\n",
    "        text = text.strip('\\n').split(' ')\n",
    "        x = np.array([train_data.pny_vocab.index(pny) for pny in text])\n",
    "        x = x.reshape(1, -1)\n",
    "        preds = sess.run(lm.preds, {lm.x: x})\n",
    "        got = ''.join(train_data.han_vocab[idx] for idx in preds[0])\n",
    "        print('原文汉字：', ''.join(train_data.han_vocab[idx] for idx in y[0]))\n",
    "        print('识别结果：', got)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
